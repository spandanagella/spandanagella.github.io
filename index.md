---
layout: single
title: ""
author_profile: true
classes: wide
---

# About Me

I am a **Research Scientist** and **Sr. Research Manager** of the Enterprise Reliability at [ServiceNow AI Research](https://www.servicenow.com/research/). My research focuses on building robust and safe, secure and robust foundation models, with interests spanning multimodal understanding, document AI, AI safety, and autonomous agents.

I received my **Ph.D. in Computer Science** from the **University of Edinburgh**, UK, where I was advised by [Prof. Mirella Lapata](https://homepages.inf.ed.ac.uk/mlap/) and [Prof. Frank Keller](https://homepages.inf.ed.ac.uk/keller/). Prior to ServiceNow, I worked at **Amazon AI** and **Alexa AI** for 6 years, and interned at **Meta AI Research** and **Microsoft Research**.

I am an active member of the academic communityâ€”I have organized workshops (RepL4NLP 2018-2022), shared tasks, and served as Area Chair and Program Committee member at major NLP/ML conferences.

---

## Research Interests

- **Multimodal Foundation Models**: Vision-language models, document understanding, chart reasoning
- **AI Safety**: Safe deployment of autonomous agents, evaluating agent safety
- **Dialogue Systems**: Task-oriented dialogue, embodied agents, conversational AI
- **Representation Learning**: Cross-lingual and multimodal representations

---

## News

- **2026**: [StarFlow](https://arxiv.org/abs/2503.21889): Generating Structured Workflow Outputs From Sketch Images accepted at EACL 2026 as main conference paper
- **2025**: Multiple papers accepted at NeurIPS, ICML, ICLR, EMNLP, and COLM!
- **2025**: [SafeArena](https://github.com/McGill-NLP/safearena) benchmark for evaluating web agent safety accepted at ICML 2025
- **2025**: [BigDocs](https://github.com/ServiceNow/stardoc) dataset accepted at ICLR 2025

---

## Selected Publications

See my [Google Scholar](https://scholar.google.com/citations?user=fChTW6MAAAAJ) for a complete list.

<div style="display: flex; align-items: flex-start; margin-bottom: 20px;">
  <div style="margin-right: 15px; flex-shrink: 0;">
    <a href="https://arxiv.org/abs/2503.21889"><img src="/assets/images/papers/2503.21889.png" alt="StarFlow" style="width: 150px; border: 1px solid #ddd; border-radius: 4px;"></a>
  </div>
  <div>
    <strong>StarFlow: Generating Structured Workflow Outputs From Sketch Images</strong><br>
    Patrice Bechard, Chao Wang, Amirhossein Abaskohi, Juan Rodriguez, Christopher Pal, David Vazquez, <strong>Spandana Gella</strong>, Sai Rajeswar, Perouz Taslakian<br>
    <em>EACL 2026</em> <a href="https://arxiv.org/abs/2503.21889">[paper]</a>
  </div>
</div>

<div style="display: flex; align-items: flex-start; margin-bottom: 20px;">
  <div style="margin-right: 15px; flex-shrink: 0;">
    <a href="https://arxiv.org/abs/2503.04957"><img src="/assets/images/papers/2503.04957.png" alt="SafeArena" style="width: 150px; border: 1px solid #ddd; border-radius: 4px;"></a>
  </div>
  <div>
    <strong>SafeArena: Evaluating the Safety of Autonomous Web Agents</strong><br>
    Ada Tur, Nicholas Meade, ..., <strong>Spandana Gella</strong>, Karolina Stanczak, Siva Reddy<br>
    <em>ICML 2025</em> <a href="https://arxiv.org/abs/2503.04957">[paper]</a> <a href="https://github.com/McGill-NLP/safearena">[code]</a>
  </div>
</div>

<div style="display: flex; align-items: flex-start; margin-bottom: 20px;">
  <div style="margin-right: 15px; flex-shrink: 0;">
    <a href="https://arxiv.org/abs/2503.15661"><img src="/assets/images/papers/2503.15661.png" alt="UI-Vision" style="width: 150px; border: 1px solid #ddd; border-radius: 4px;"></a>
  </div>
  <div>
    <strong>UI-Vision: A Desktop-centric GUI Benchmark for Visual Perception and Interaction</strong><br>
    Shravan Nayak, ..., <strong>Spandana Gella</strong>, Sai Rajeswar Mudumba<br>
    <em>ICML 2025</em> <a href="https://arxiv.org/abs/2503.15661">[paper]</a> <a href="https://github.com/uivision/UI-Vision">[code]</a>
  </div>
</div>

<div style="display: flex; align-items: flex-start; margin-bottom: 20px;">
  <div style="margin-right: 15px; flex-shrink: 0;">
    <a href="https://arxiv.org/abs/2412.04626"><img src="/assets/images/papers/2412.04626.png" alt="BigDocs" style="width: 150px; border: 1px solid #ddd; border-radius: 4px;"></a>
  </div>
  <div>
    <strong>BigDocs: An Open and Permissively-Licensed Dataset for Training Multimodal Models</strong><br>
    Juan A. Rodriguez, ..., <strong>Spandana Gella</strong>, et al.<br>
    <em>ICLR 2025</em> <a href="https://arxiv.org/abs/2412.04626">[paper]</a> <a href="https://github.com/ServiceNow/stardoc">[code]</a>
  </div>
</div>

<div style="display: flex; align-items: flex-start; margin-bottom: 20px;">
  <div style="margin-right: 15px; flex-shrink: 0;">
    <a href="https://arxiv.org/abs/2502.01341"><img src="/assets/images/papers/2502.01341.png" alt="AlignVLM" style="width: 150px; border: 1px solid #ddd; border-radius: 4px;"></a>
  </div>
  <div>
    <strong>AlignVLM: Bridging Vision and Language Latent Spaces for Multimodal Understanding</strong><br>
    Ahmed Masry, ..., <strong>Spandana Gella</strong>, Sai Rajeswar Mudumba<br>
    <em>NeurIPS 2025</em> <a href="https://arxiv.org/abs/2502.01341">[paper]</a>
  </div>
</div>

---

## Contact

Feel free to reach out via email or connect on LinkedIn!
